<!DOCTYPE html>
<html>

<head>
    <title>Prompting in Vision</title>

    <style>
        body {
            width: 60%;
            margin: 0 auto;
            background-color: #f2f2f2;
        }

        a:link {
            color: blue;
        }

        a:visited {
            color: blue;
        }

        a:hover {
            color: orange;
        }

        img {
            max-width: 100%;
            max-height: 100%;
        }

        td {
            text-align: left;
            padding-top: 5px;
            padding-bottom: 5px;
            padding-left: 15px;
            padding-right: 0px;
        }

        .row {
          content: "";
          clear: both;
          display: flex;
          justify-content: center
        }

        .column {
          float: left;
          width: 22%;
          padding: 5px;
        }

        .container {
            text-align: center;
            background-color: #ffffff;
            padding: 15px;
            margin: 20px;
        }

        .overview {
            text-align: left;
        }

        @media only screen and (max-width: 768px) {
          .row {
              display: block;
          }

          .column {
              width: 100%;
          }
        }
    </style>

    <meta charset="UTF-8">
    <meta name="description" content="CVPR 2023 Tutorial on Prompting in Vision">
    <meta name="keywords" content="CVPR, Computer Vision, Prompting">
    <meta name="author" content="Kaiyang Zhou et al.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

<div class="container">
    <h2>CVPR 2023 Tutorial</h2>
    <h1><i>Prompting in Vision</i></h1>
    <h2>XX June 2023</h2>
    <img src="images/cvpr_banner.png" width="50%" alt="CVPR2023">
</div>

<div class="container">
    <h2>Tutorial lecturers</h2>

    <div class="row">
        <div class="column">
            <a href="https://kaiyangzhou.github.io/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Kaiyang Zhou<br>NTU</div>
            </a>
        </div>
        <div class="column">
            <a href="https://liuziwei7.github.io/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Ziwei Liu<br>NTU</div>
            </a>
        </div>
        <div class="column">
            <a href="http://web.mit.edu/phillipi/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Phillip Isola<br>MIT</div>
            </a>
        </div>
        <div class="column">
            <a href="https://hjbahng.github.io/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Hyojin Bahng<br>MIT</div>
            </a>
        </div>
    </div>

    <div class="row">
        <div class="column">
            <a href="https://people.csail.mit.edu/ludwigs/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Ludwig Schmidt<br>University of Washington</div>
            </a>
        </div>
        <div class="column">
            <a href="https://sarahpratt.github.io/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Sarah Pratt<br>University of Washington</div>
            </a>
        </div>
        <div class="column">
            <a href="https://www.jasonwei.net/" target="_blank">
                <img src="people/xx.jpg" alt="xx">
                <div>Jason Wei<br>Google Brain</div>
            </a>
        </div>
    </div>
</div>

<div class="container">
    <h2>Overview</h2>
    <div class="overview">
        <p>In natural language processing, prompting has been widely used to adapt large-scale pre-trained models like GPT-3 or BERT. This is done by adding a small number of hand-crafted or learnable parameters to a model’s input space without modifying any pre-trained weight. Such a paradigm has sparked huge interests in the vision domain, which is largely motivated by the dramatic scaling of vision models in terms of size, data and modality that has made downstream adaptation a challenging problem.</p>

        <p>In the context of computer vision, the concept of prompting has been explored in various ways: learning some input tokens for a Transformer encoder; prompting a vision-language model for content editing or to perform zero-shot, open-vocabulary recognition; etc. The potential of prompting methods has been demonstrated in wide-ranging problem domains, e.g., image classification, object detection, semantic segmentation, and image generation and editing.</p>

        <p>In this tutorial, we aim to give a comprehensive background on prompting—by building connections between research in vision and language—and review the latest studies that design prompting methods to tackle computer vision problems. We will also cover topics related to large-scale vision models, such as pre-training and weights interpolation.</p>
    </div>
</div>

<div class="container">
    <h2>Schedule</h2>
    <table>
        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Opening remarks</td>
        </tr>
        
        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Prompt learning for visual recognition and generation</td>
        </tr>

        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Visual prompting</td>
        </tr>

        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Break</td>
        </tr>

        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Improved model adaptation via weight interpolation and prompt generation</td>
        </tr>

        <tr>
            <td>08:00 am - 08:00 am</td>
            <td>Prompting language-vision and multimodal models</td>
        </tr>
    </table>
</div>

<br>

</body>
</html>
